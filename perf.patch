--- a/Scripts/get_sentences.py
+++ b/Scripts/get_sentences.py
@@
 import zipfile
 import time
+from concurrent.futures import ProcessPoolExecutor, as_completed
 try:
     import requests  # type: ignore
 except Exception:  # pragma: no cover
     requests = None
 from pathlib import Path
-from typing import Iterable, List, Optional, Tuple, Dict
+from typing import Iterable, List, Optional, Tuple, Dict, Set
@@
 QUOTE_RE = re.compile(r"[\"“”‘’]")
+_WORD_RE = re.compile(r"[A-Za-z']+$")
@@
 VERB_LIKE = set("""
 is are was were be been being have has had do does did can could will would shall should may might must
 go goes went gone make makes made say says said see sees saw seen know knows knew known think thinks thought
 come comes came come take takes took taken give gives gave given tell tells told ask asks asked want wants wanted
 """.split())
+
+STOPWORDS = FUNC_WORDS  # alias for readability
@@
 def extract_text_from_file(path: Path) -> str:
     lower = path.name.lower()
     if lower.endswith(".txt"):
         return read_txt(path)
     if lower.endswith(".epub"):
         return read_epub(path)
     # (Optional) PDF support could be added with PyPDF2 if installed.
     return ""
+
+def _extract_and_split_one(fp: Path) -> Tuple[Path, List[str]]:
+    text = extract_text_from_file(fp)
+    if not text:
+        return (fp, [])
+    sents = split_sentences(text)
+    return (fp, sents)
@@
-def search_sources_for_term(
-    term: str,
-    file_sentences: Dict[Path, List[str]],
-    min_words: int,
-    max_words: int,
-    prefer_shorter: bool = True,
-    inverted: Optional[Dict[str, List[Tuple[Path, int]]]] = None,
-) -> Optional[str]:
-    """
-    Search using an inverted index when available to avoid scanning all sentences.
-    Fallback to full scan if the term has no postings.
-    """
-    term_re = build_match_regex(term)
-    best: Optional[str] = None
-    best_len: int = 10**9
-    candidates: List[Tuple[Path, int]] = []
-
-    if inverted is not None:
-        seen: set = set()
-        for v in _term_variants(term):
-            for ref in inverted.get(v, []):
-                if ref not in seen:
-                    seen.add(ref)
-                    candidates.append(ref)
-
-    if candidates:
-        # Iterate only candidate sentences
-        for fp, si in candidates:
-            s = file_sentences.get(fp, [])
-            if si >= len(s):
-                continue
-            s2 = strip_outer_quotes(s[si])
-            s2 = first_sentence(s2)
-            if term_re.search(s2) and seems_complete_sentence(s2, min_words, max_words):
-                if prefer_shorter:
-                    n = len(s2.split())
-                    if n < best_len:
-                        best, best_len = s2, n
-                else:
-                    return s2
-        return best
-
-    # Fallback: rare word or OOV — scan all sentences
-    for fp, sentences in file_sentences.items():
-        if not sentences:
-            continue
-        for s in sentences:
-            s2 = strip_outer_quotes(s)
-            s2 = first_sentence(s2)
-            if term_re.search(s2) and seems_complete_sentence(s2, min_words, max_words):
-                if prefer_shorter:
-                    n = len(s2.split())
-                    if n < best_len:
-                        best, best_len = s2, n
-                else:
-                    return s2
-    return best
+def search_sources_for_term(
+    term: str,
+    file_sentences: Dict[Path, List[str]],
+    min_words: int,
+    max_words: int,
+    prefer_shorter: bool = True,
+    inverted: Optional[Dict[str, List[Tuple[int, int]]]] = None,
+    files_list: Optional[List[Path]] = None,
+) -> Optional[str]:
+    """
+    Search using an inverted index when available to avoid scanning all sentences.
+    Fallback to full scan if the term has no postings.
+    """
+    term_re = build_match_regex(term)
+    best: Optional[str] = None
+    best_len: int = 10**9
+    candidates: List[Tuple[int, int]] = []
+
+    if inverted is not None:
+        seen: set = set()
+        for v in _term_variants(term):
+            for ref in inverted.get(v, []):
+                if ref not in seen:
+                    seen.add(ref)
+                    candidates.append(ref)
+
+    if candidates and files_list is not None:
+        # Iterate only candidate sentences (already split and pre-filtered)
+        for fid, si in candidates:
+            fp = files_list[fid]
+            s_list = file_sentences.get(fp, [])
+            if si >= len(s_list):
+                continue
+            s2 = strip_outer_quotes(s_list[si])  # already a single sentence
+            if term_re.search(s2):
+                if prefer_shorter:
+                    n = len(s2.split())
+                    if n < best_len:
+                        best, best_len = s2, n
+                else:
+                    return s2
+        return best
+
+    # Fallback: rare word or OOV — scan all sentences (already split)
+    for fp, sentences in file_sentences.items():
+        if not sentences:
+            continue
+        for s in sentences:
+            s2 = strip_outer_quotes(s)
+            if term_re.search(s2) and seems_complete_sentence(s2, min_words, max_words):
+                if prefer_shorter:
+                    n = len(s2.split())
+                    if n < best_len:
+                        best, best_len = s2, n
+                else:
+                    return s2
+    return best
@@
     ap.add_argument("--user-agent",
                     help="User-Agent header for API requests. If omitted, uses the TATOEBA_USER_AGENT env var. Required when --tatoeba-fallback is set.")
+    ap.add_argument("--workers", type=int, default=min(6, (os.cpu_count() or 4)),
+                    help="Parallel workers for file decoding/splitting (default: up to 6).")
+    ap.add_argument("--index-only-terms", action="store_true",
+                    help="Only index tokens that match CSV terms (plus simple variants); reduces memory and speeds lookups.")
@@
     header, rows = load_csv_rows(input_csv)
 
+    # Optional whitelist of tokens to index (dramatically smaller/faster)
+    term_whitelist: Optional[Set[str]] = None
+    if args.index_only_terms:
+        terms: List[str] = []
+        for row in rows:
+            t = (row.get("English_Term") or "").strip()
+            if t:
+                terms.append(t.lower())
+        wl: Set[str] = set()
+        for t in terms:
+            wl.add(t)
+            wl.add(f"{t}'s")
+            wl.add(f"{t}s")
+            wl.add(f"{t}es")
+        wl = {w for w in wl if w not in STOPWORDS}
+        term_whitelist = wl
@@
-    print(f"[INIT] Preloading {len(files)} files…")
-    file_sentences: Dict[Path, List[str]] = {}
-    total_sentences = 0
-    for fp in files:
-        text = extract_text_from_file(fp)
-        if not text:
-            file_sentences[fp] = []
-            continue
-        sents = split_sentences(text)
-        file_sentences[fp] = sents
-        total_sentences += len(sents)
-    print(f"[INIT] Ready. {total_sentences} sentences indexed across {len(files)} files.")
+    print(f"[INIT] Preloading {len(files)} files…")
+    file_sentences: Dict[Path, List[str]] = {}
+    total_sentences = 0
+    workers = max(1, int(args.workers))
+    with ProcessPoolExecutor(max_workers=workers) as ex:
+        futs = [ex.submit(_extract_and_split_one, fp) for fp in files]
+        for fut in as_completed(futs):
+            fp, sents = fut.result()
+            file_sentences[fp] = sents
+            total_sentences += len(sents)
+    print(f"[INIT] Ready. {total_sentences} sentences indexed across {len(files)} files (workers={workers}).")
@@
-    print("[INIT] Building inverted index…")
-    from collections import defaultdict
-    inverted: Dict[str, List[Tuple[Path, int]]] = defaultdict(list)
-    for fp, sents in file_sentences.items():
-        for si, s in enumerate(sents):
-            # use unique tokens per sentence to keep postings lists smaller
-            toks = set(t.lower() for t in _tokenize_simple(s) if re.match(r"[A-Za-z']+$", t))
-            for t in toks:
-                inverted[t].append((fp, si))
-    print(f"[INIT] Indexed {len(inverted)} unique tokens.")
+    files_list: List[Path] = list(files)
+    file_id_of: Dict[Path, int] = {fp: i for i, fp in enumerate(files_list)}
+
+    print("[INIT] Building inverted index…")
+    from collections import defaultdict
+    inverted: Dict[str, List[Tuple[int, int]]] = defaultdict(list)
+    min_w, max_w = args.min_words, args.max_words
+    for fp, sents in file_sentences.items():
+        fid = file_id_of[fp]
+        for si, s in enumerate(sents):
+            # pre-filter to acceptable sentences to speed up lookups later
+            if not seems_complete_sentence(s, min_words=min_w, max_words=max_w):
+                continue
+            # unique tokens per sentence, letters/apostrophes only
+            toks = {t.lower() for t in _tokenize_simple(s) if _WORD_RE.fullmatch(t)}
+            if term_whitelist is not None:
+                hits = toks & term_whitelist
+                if not hits:
+                    continue
+                for t in hits:
+                    inverted[t].append((fid, si))
+            else:
+                for t in toks:
+                    inverted[t].append((fid, si))
+    print(f"[INIT] Indexed {len(inverted)} unique tokens (postings pre-filtered).")
@@
-            sentence = search_sources_for_term(
+            sentence = search_sources_for_term(
                 term=term,
                 file_sentences=file_sentences,
                 min_words=args.min_words,
                 max_words=args.max_words,
                 prefer_shorter=args.prefer_shorter,
-                inverted=inverted,
+                inverted=inverted,
+                files_list=files_list,
             )
             cache[key] = sentence
